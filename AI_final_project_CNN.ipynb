{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_final_project_CNN",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ben900926/Plant-seedling-classification/blob/main/AI_final_project_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QVJlU0qPCv5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c0f41f6-d456-492d-ea29-23c3bcd7641d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e-hsaKpwJ7a1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # GRAPHING AND VISUALIZATIONS\n",
        "import os\n",
        "import cv2\n",
        "from sklearn import preprocessing # for data preprocess, e.g label encoding\n",
        "from sklearn.model_selection import train_test_split # for splitting train data for validation\n",
        "# for convolutional\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,CSVLogger\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# convolution end\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scale = 70\n",
        "seed = 7"
      ],
      "metadata": {
        "id": "NmKfoe3lXy1p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Image_dataset:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.train_set = list()\n",
        "        self.original_labels = list()\n",
        "        self.train_labels = list() # store files address for testing\n",
        "        self.x_train = []\n",
        "        self.x_val = []\n",
        "        self.y_train = []\n",
        "        self.y_val = []\n",
        "\n",
        "    # load training sets from image files\n",
        "    def load_train_file(self,path):\n",
        "        dirs = os.listdir(path) # read images from given file\n",
        "        for dir in dirs:\n",
        "            count = 0\n",
        "            for img in os.listdir(os.path.join(path,dir)):\n",
        "                img = os.path.join(path,dir,img)\n",
        "                self.train_set.append(cv2.resize(cv2.imread(img),(scale,scale)))\n",
        "                self.train_labels.append(dir)\n",
        "                count += 1\n",
        "            print(f\"{dir}: load {count} images done!\")\n",
        "        self.train_set = np.asarray(self.train_set)\n",
        "        print('\\n')\n",
        "        print('Orignal xtrain shape : {}'.format(self.train_set.shape)) #(4750, dim, dim, 3)\n",
        "        self.train_labels = pd.DataFrame(self.train_labels)\n",
        "    \n",
        "    # load testing sets from single directory\n",
        "    def load_test_file(self,path):\n",
        "      dir = os.listdir(path)\n",
        "      count = 0\n",
        "      for img in dir:\n",
        "        self.train_labels.append(img)\n",
        "        img = os.path.join(path,img)\n",
        "        self.train_set.append(cv2.resize(cv2.imread(img),(scale,scale)))\n",
        "        count += 1\n",
        "      print(f\"load {count} images done!\")\n",
        "      self.train_set = np.asarray(self.train_set)\n",
        "      print('\\n')\n",
        "      print('Test xtrain shape : {}'.format(self.train_set.shape))\n",
        "    \n",
        "    # convert image to hsv, remove background and noise\n",
        "    def clean_img(self):\n",
        "        new_train = []\n",
        "        for i in self.train_set:\n",
        "            blurr = cv2.GaussianBlur(i,(5,5),0)\n",
        "            hsv = cv2.cvtColor(blurr,cv2.COLOR_BGR2HSV)\n",
        "            # GREEN PARAMETERS\n",
        "            lower = (25,40,50)\n",
        "            upper = (75,255,255)\n",
        "            mask = cv2.inRange(hsv,lower,upper)\n",
        "            struc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(11,11))\n",
        "            mask = cv2.morphologyEx(mask,cv2.MORPH_CLOSE,struc)\n",
        "            boolean = mask>0\n",
        "            new = np.zeros_like(i,np.uint8)\n",
        "            new[boolean] = i[boolean]\n",
        "            new_train.append(new)\n",
        "        self.train_set = np.asarray(new_train)\n",
        "    \n",
        "    # use sklearn to do label encoding\n",
        "    def label_encoding(self):\n",
        "        labels = preprocessing.LabelEncoder()\n",
        "        labels.fit(self.train_labels[0]) # collect label from the datasets of images\n",
        "        self.original_labels = labels\n",
        "        # tranform label into binary format\n",
        "        encoded_label = labels.transform(self.train_labels[0])\n",
        "        print('\\n')\n",
        "        print('Classes'+str(labels.classes_))\n",
        "        binary_label = np_utils.to_categorical(encoded_label)\n",
        "        self.train_labels = binary_label    \n",
        "\n",
        "    # split the train data to prevent overfitting\n",
        "    def split_data(self):\n",
        "      # normalization\n",
        "      self.train_set = self.train_set/255\n",
        "      self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.train_set, self.train_labels, test_size=0.1, random_state=seed, stratify=self.train_labels)\n",
        "      print('\\n---Split training and validation sets---')\n",
        "      print('xtrain shape : {}'.format(self.x_train.shape))\n",
        "      print('ytrain shape : {}'.format(self.y_train.shape))\n",
        "      print('  xval shape : {}'.format(self.x_val.shape))\n",
        "      print('  yval shape : {}'.format(self.y_val.shape))\n",
        "      print('----------------------------------------------')\n",
        "      # save the datasets\n",
        "      np.savez('/content/drive/MyDrive/AI_final/plant_split_dataset.npz',\n",
        "               train_set=self.train_set,train_labels=self.train_labels,x_train=self.x_train,x_val=self.x_val,y_train=self.y_train,y_val=self.y_val)\n"
      ],
      "metadata": {
        "id": "Bj3E0C8ZB5BQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class convolutional_network:\n",
        "\n",
        "  def __init__(self, epoches, batch_size):\n",
        "    dataset = np.load('/content/drive/MyDrive/AI_final/plant_split_dataset.npz') # load dataset from file\n",
        "    self.train_set = dataset['train_set']\n",
        "    self.train_labels = dataset['train_labels']\n",
        "    self.x_train = dataset['x_train']\n",
        "    self.x_val = dataset['x_val']\n",
        "    self.y_train = dataset['y_train']\n",
        "    self.y_val = dataset['y_val']\n",
        "    #self.datagen = None\n",
        "    # for training\n",
        "    self.epoches = epoches\n",
        "    self.batch_size = batch_size\n",
        "    # define neural network\n",
        "    self.model = Sequential()\n",
        "  # construct neural network structure\n",
        "  def network_layer(self):\n",
        "    np.random.seed(seed)\n",
        "    # add layers : 4 convolution layers and 3 fully connected layers\n",
        "    self.model.add(Conv2D(filters=64, kernel_size=(5, 5), input_shape=(scale, scale, 3), activation='relu'))\n",
        "    self.model.add(BatchNormalization(axis=3))\n",
        "    # convolution + pooling layer\n",
        "    self.model.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu'))\n",
        "    self.model.add(MaxPooling2D((2, 2))) # resist noise and reduce computation resources\n",
        "    self.model.add(BatchNormalization(axis=3))\n",
        "    self.model.add(Dropout(0.1)) # ignored 0.1 neural to prevent overfitting\n",
        "    self.model.add(Conv2D(filters=128, kernel_size=(5, 5), activation='relu'))\n",
        "    self.model.add(BatchNormalization(axis=3))\n",
        "    self.model.add(Conv2D(filters=128, kernel_size=(5, 5), activation='relu'))\n",
        "    self.model.add(MaxPooling2D((2, 2)))\n",
        "    self.model.add(BatchNormalization(axis=3))\n",
        "    self.model.add(Dropout(0.1))\n",
        "    self.model.add(Conv2D(filters=256, kernel_size=(5, 5), activation='relu'))\n",
        "    self.model.add(BatchNormalization(axis=3))\n",
        "    self.model.add(Conv2D(filters=256, kernel_size=(5, 5), activation='relu'))\n",
        "    self.model.add(MaxPooling2D((2, 2)))\n",
        "    self.model.add(BatchNormalization(axis=3))\n",
        "    self.model.add(Dropout(0.1))\n",
        "    self.model.add(Flatten()) # flatten map to a single vector\n",
        "    self.model.add(Dense(256, activation='relu')) # fully connected networks\n",
        "    self.model.add(BatchNormalization())\n",
        "    self.model.add(Dropout(0.5))\n",
        "    self.model.add(Dense(256, activation='relu'))\n",
        "    self.model.add(BatchNormalization())\n",
        "    self.model.add(Dropout(0.5))\n",
        "    self.model.add(Dense(self.train_labels.shape[1] # length of classes\n",
        "                         , activation='softmax'))\n",
        "    self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    #model.summary() # output result\n",
        "\n",
        "  # train the model for given epoches\n",
        "  def fit(self,save_best_path,save_full_path):\n",
        "    # prevent overfitting\n",
        "    # ImageDataGenerator() randomly changes the characteristics of images and provides randomness in the data and makes dataset bigger?\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=180,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.3, # Randomly zoom image\n",
        "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True, # randomly flip images\n",
        "        )\n",
        "    print('xtrain shape : {}'.format(self.x_train.shape))\n",
        "    datagen.fit(self.x_train)\n",
        "    # reduce the learning rate by 0.5\n",
        "    lrr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=6, verbose=1, min_lr=0.00001)\n",
        "    # save the best considered model with epoch number and validation accuracy\n",
        "    checkpoints = ModelCheckpoint(save_best_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "    # save full model\n",
        "    checkpoints_full = ModelCheckpoint(save_full_path, monitor='val_accuracy', verbose=1, save_best_only=False, mode='max')\n",
        "    # train model with batch\n",
        "    print(\"batch size:{}\".format(self.x_train.shape[0]))\n",
        "    # step_per_epoch * batch_size = len(train_data)\n",
        "    train_model = self.model.fit_generator( datagen.flow(self.x_train,self.y_train,batch_size=self.batch_size)\n",
        "                                 ,epochs=self.epoches\n",
        "                                 ,validation_data=(self.x_val,self.y_val)\n",
        "                                ,steps_per_epoch=self.x_train.shape[0]/self.batch_size # batch size\n",
        "                                 #,validation_steps=len(self.x_val)\n",
        "                                 ,callbacks=[checkpoints, lrr, checkpoints_full])\n",
        "  # evalutate the model performance\n",
        "  def evaluate_model(self):\n",
        "    # load the best model\n",
        "    self.model.load_weights('/content/drive/MyDrive/AI_final/weights.best_77-0.90.hdf5')\n",
        "    # print evaluation\n",
        "    print(self.model.evaluate(self.x_train,self.y_train))\n",
        "    print(self.model.evaluate(self.x_val,self.y_val))\n",
        "    # use confusion matrix to evaluate the prediction\n",
        "    Y_true = np.argmax(self.y_val, axis=1) # ground truth\n",
        "    Y_pred = self.model.predict(self.x_val) # prediction\n",
        "    Y_pred_class = np.argmax(Y_pred, axis=1) # predict the class\n",
        "    cmatrix = confusion_matrix(Y_true, Y_pred_class)\n",
        "    print(\"\\n confusion matrix:\")\n",
        "    print(cmatrix)\n",
        "    "
      ],
      "metadata": {
        "id": "BgfkpxLHhI2c"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train = Image_dataset()\n",
        "    train.load_train_file('/content/drive/MyDrive/AI_final/train')\n",
        "    train.clean_img()\n",
        "    train.label_encoding()\n",
        "    train.split_data()\n",
        "    "
      ],
      "metadata": {
        "id": "sPqFTX5KEdWc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f085c5ab-4f88-47fc-bef2-c00c241445df"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sugar beet: load 385 images done!\n",
            "Small-flowered Cranesbill: load 496 images done!\n",
            "Maize: load 221 images done!\n",
            "Shepherds Purse: load 231 images done!\n",
            "Loose Silky-bent: load 654 images done!\n",
            "Fat Hen: load 475 images done!\n",
            "Cleavers: load 287 images done!\n",
            "Scentless Mayweed: load 516 images done!\n",
            "Common wheat: load 221 images done!\n",
            "Common Chickweed: load 611 images done!\n",
            "Charlock: load 390 images done!\n",
            "Black-grass: load 263 images done!\n",
            "\n",
            "\n",
            "Orignal xtrain shape : (4750, 70, 70, 3)\n",
            "\n",
            "\n",
            "Classes['Black-grass' 'Charlock' 'Cleavers' 'Common Chickweed' 'Common wheat'\n",
            " 'Fat Hen' 'Loose Silky-bent' 'Maize' 'Scentless Mayweed'\n",
            " 'Shepherds Purse' 'Small-flowered Cranesbill' 'Sugar beet']\n",
            "\n",
            "---Split training and validation sets---\n",
            "xtrain shape : (4275, 70, 70, 3)\n",
            "ytrain shape : (4275, 12)\n",
            "  xval shape : (475, 70, 70, 3)\n",
            "  yval shape : (475, 12)\n",
            "----------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    \n",
        "    train2 = convolutional_network(epoches=80,batch_size=128)\n",
        "    train2.network_layer()\n",
        "    "
      ],
      "metadata": {
        "id": "vPKhvWmHzg-g"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # train for a long time!\n",
        "    train2.fit(\"drive/MyDrive/AI_final/weights.best_{epoch:02d}-{val_accuracy:.2f}.hdf5\",\"drive/MyDrive/AI_final/weights.last_auto4.hdf5\")"
      ],
      "metadata": {
        "id": "5gjizuVPTwqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    train2.evaluate_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j38UrSLxT1wB",
        "outputId": "adba7dad-0e21-47cf-b834-b83e6a33f71e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "134/134 [==============================] - 128s 946ms/step - loss: 0.2364 - accuracy: 0.9120\n",
            "[0.23635384440422058, 0.9120467901229858]\n",
            "15/15 [==============================] - 14s 934ms/step - loss: 0.2880 - accuracy: 0.9032\n",
            "[0.2879680395126343, 0.9031578898429871]\n",
            "\n",
            " confusion matrix:\n",
            "[[11  0  0  0  2  0 13  0  0  0  0  0]\n",
            " [ 0 39  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  3 23  0  0  0  0  0  0  0  3  0]\n",
            " [ 0  0  0 57  0  0  0  1  2  0  0  1]\n",
            " [ 1  0  0  0 20  0  0  1  0  0  0  0]\n",
            " [ 1  0  0  0  0 44  1  1  0  0  0  0]\n",
            " [ 5  0  0  0  1  0 58  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0 22  0  0  0  0]\n",
            " [ 0  0  1  0  0  1  1  0 48  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0  2 21  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0 50  0]\n",
            " [ 0  1  0  0  0  0  0  2  0  0  0 36]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    test = Image_dataset()\n",
        "    test.load_test_file('/content/drive/MyDrive/AI_final/test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5oATFEIsKnX",
        "outputId": "51414848-c2c7-4ece-a741-73646eae9e81"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load 794 images done!\n",
            "\n",
            "\n",
            "Test xtrain shape : (794, 70, 70, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    test.clean_img()\n",
        "    # predict the test files\n",
        "    test.train_set = test.train_set/255 # normalization\n",
        "    prediction = train2.model.predict(test.train_set)\n",
        "    # prediction to csv file\n",
        "    predict = np.argmax(prediction, axis=1)\n",
        "    pred_label = train.original_labels.classes_[predict]\n",
        "    result = {'file':test.train_labels, 'species':pred_label}\n",
        "    result = pd.DataFrame(result)\n",
        "    result.to_csv(\"/content/drive/MyDrive/AI_final/Prediction.csv\", index=False)"
      ],
      "metadata": {
        "id": "x5981sLsuDYe"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}